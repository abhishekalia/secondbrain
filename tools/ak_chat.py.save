# tools/ak_chat.py
import streamlit as st
import traceback

from core.llm import chat, LLMError
from core.prompt import build_messages
try:
    from core import memory
except Exception:
    memory = None

st.set_page_config(page_title="AKtivate.ai", page_icon="âš¡", layout="wide")
st.title("AK â€” Second Brain (Local)")

with st.sidebar:
    st.caption("AKtivate.ai â€¢ Local Jarvis")
    if st.button("New Chat"):
        st.session_state["history"] = []

history = st.session_state.setdefault("history", [])

for role, content in history:
    with st.chat_message(role):
        st.markdown(content)

user_text = st.chat_input("Typeâ€¦ (one-liners preferred)")
if not user_text:
    st.stop()

history.append(("user", user_text))
with st.chat_message("user"):
    st.markdown(user_text)

ctx_snippets = []
if memory:
    try:
        last_j = memory.get_last_journal(embedder=None, k=1)
        if last_j:
            ctx_snippets.append(f"Last journal: {last_j[0][:400]}")
    except Exception:
        pass

trunc = history[-16:]
user_as_text = trunc[-1][1] if trunc else user_text
msgs = build_messages(user_as_text, context_snippets=ctx_snippets)

try:
    reply = chat(msgs)
except LLMError as e:
    reply = f"LLM error: {e}\n\n**Next step:** run the curl health check and ensure Ollama is serving."
except Exception:
    reply = "Unhandled error:\n\n```\n" + traceback.format_exc() + "\n```"

with st.chat_message("assistant"):
    st.markdown(reply)
history.append(("assista# tools/ak_chat.py
import streamlit as st
import traceback

from core.llm import chat, LLMError
from core.prompt import build_messages
# optional: lightweight journal context (safe if file missing)
try:
    from core import memory
except Exception:
    memory = None

st.set_page_config(page_title="AKtivate.ai", page_icon="âš¡", layout="wide")
st.title("AK â€” Second Brain (Local)")

# Sidebar
with st.sidebar:
    st.caption("AKtivate.ai â€¢ Local Jarvis")
    if st.button("New Chat"):
        st.session_state["history"] = []

# Session chat history: list[tuple(role, content)]
history = st.session_state.setdefault("history", [])

# Render history
for role, content in history:
    with st.chat_message(role):
        st.markdown(content)

# Chat input
user_text = st.chat_input("Typeâ€¦ (one-liners preferred)")
if not user_text:
    st.stop()

# Show user bubble immediately
history.append(("user", user_text))
with st.chat_message("user"):
    st.markdown(user_text)

# Build lightweight context snippets (optional/journals)
ctx_snippets = []
if memory:
    try:
        last_j = memory.get_last_journal(embedder=None, k=1)
        if last_j:
            ctx_snippets.append(f"Last journal: {last_j[0][:400]}")
    except Exception:
        pass

# Take last 16 turns to form context
trunc = history[-16:]
user_as_text = trunc[-1][1] if trunc else user_text
msgs = build_messages(user_as_text, context_snippets=ctx_snippets)

# Model call with error surfacing
try:
    reply = chat(msgs)
except LLMError as e:
    reply = f"LLM error: {e}\n\n**Next step:** run the curl health check and ensure Ollama is serving."
except Exception as e:
    reply = "Unhandled error:\n\n```\n" + traceback.format_exc() + "\n```"

# Show assistant bubble and save
with st.chat_message("assistant"):
    st.markdown(reply)
history.append(("assistant", reply))
import streamlit as st, requests, re, json

def _set_last_detail(text:str):
    st.session_state['last_detail'] = text or ''
# tools/ak_chat.py
import streamlit as st
import traceback

from core.llm import chat, LLMEr
def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())


def _get_last_detail():
    return st.session_state.get('last_detail','')


def _one_line(x):
    return ' '.join(str(x).splitlines())

'''    st.caption("AKtivate.ai â€¢ Local Jarvis"    if st.button("New Chat"):        st.session_state["history"] = []history = st.session_state.setdefault("history", [])afor role, content in history:    with st.chat_message(role)        st.markdown(content)user_text = st.chat_input("Typeâ€¦ (one-liners preferred)")if not user_text:    st.stop()history.append(("user", user_text))with st.chat_message("user"):    st.markdown(user_text)ctx_snippets = []if memory:    try:        last_j = memory.get_last_journal(embedder=None, k=1        if last_j:            ctx_snippets.append(f"Last journal: {last_j[0][:400]    except Exceptio        ptrunc = history[-16:user_as_text = trunc[-1][1] if trunc else user_teamsgs = build_messages(user_as_text, context_snippets=ctx_snippets)try  OLLAMA_URL="http://localhost:11434"; MODEL="mistral:latest"

def chat_llm(msgs):  # LLM_HARDENED
    r=requests.post(f"%s/api/chat"%OLLAMA_URL,
        json={"model":MODEL,"messages":msgs,"stream":False,
              "options":{"num_ctx":8192,"temperature":0.2,"top_p":0.1}},
        timeout=10)
    if r.status_code!=200:
        return f"LLM HTTP {r.status_code}: {r.text[:200]}"
    j=r.json()
    if isinstance(j,dict):
        if j.get("message",{}).get("content"): return j["message"]["content"]
        msgs=j.get("messages") or []
        if msgs and isinstance(msgs[-1],dict) and msgs[-1].get("content"): return msgs[-1]["content"]

        if j.get("content"): return j["content"]
    return "LLM empty response."

st.set_page_config(page_title="Second Brain", layout="wide")
st.title("Second Brain")

with st.sidebar:
    if st.button("ðŸ†• New chat"):
        st.session_state.clear(); st.rerun()
    st.caption("Minimal UI baseline")

if "messages" not in st.session_state:
    st.session_state.messages=[]

for m in st.session_state.messages:
    with st.chat_message(m["role"]): st.markdown(m["content"])

prompt = st.chat_input("Type hereâ€¦ (Enter to send)")
if not prompt: raise SystemExit

st.session_state.messages.append({"role":"user","content":prompt})
with st.chat_message("user"): st.markdown(prompt)
low = prompt.strip().lower()  # ensure available for all fast-path handlers


# ---- deterministic fast-paths ----
# MORE_FASTPATH
if low in ("more","tell me more","expand","details","expand last"):
    detail = _get_last_detail() or st.session_state.get("last_full","")
    out = detail if detail else "No prior detail to expand."
    with st.chat_message("assistant"): st.markdown(out if len(out) <= 2000 else out[:2000] + "\nâ€¦")
    st.session_state.messages.append({"role":"assistant","content":out})
    raise SystemExit


# JOURNAL_DATE_KEYWORD_FASTPATH
m = re.search(r'^journal\s+on[:\s]+([^\n]+?)\s+(?:keyword|about|with)[:\s]+(.+)$', low)
if m:
    raw_date, kw = m.group(1).strip(), m.group(2).strip().lower()
    # normalize date: YYYY-MM-DD or "Sep 21"
    from datetime import datetime as _dt
    date_key = None
    for fmt in ("%Y-%m-%d","%b %d","%B %d"):
        try:
            dt = _dt.strptime(raw_date, fmt)
            year = dt.year if "%Y" in fmt else _dt.now().year
            date_key = f"{year:04d}-{dt.month:02d}-{dt.day:02d}"
            break
        except Exception:
            pass
    rows = []
    memlog = Path.home()/ "ak"/ "memories"/ "memories.jsonl"
    try:
        for line in memlog.read_text(encoding="utf-8").splitlines():
            try:
                rec = json.loads(line)
                if rec.get("type") == "journal":
                    ts = str(rec.get("timestamp",""))
                    txt = (rec.get("text") or "")
                    if (not date_key or ts.startswith(date_key)) and (kw in txt.lower()):
                        rows.append((ts, txt.strip()))
            except Exception:
                pass
    except FileNotFoundError:
        rows = []
    rows.sort(key=lambda x: x[0])
    if not rows:
        out = "No matching journal entries."
        details_store = ''
    else:
        bullets = []
        for ts, txt in rows[-5:]:
            bullets.append(f"- {ts} â€” {_one_line(txt)[:160]}")
        out = f"Matches for '{kw}' on {raw_date}:\n" + "\n".join(bullets)
        details_store = "\n\n".join([f"{ts}\n{txt}" for ts, txt in rows[-5:]])
    with st.chat_message("assistant"): st.markdown(out)
    st.session_state.messages.append({"role":"assistant","content":out})
    
    _set_last_detail(details_store)
raise SystemExit



# LAST_JOURNAL_FASTPATH
if low.startswith("last journal") or low == "last journal":
    memlog = Path.home()/ "ak"/ "memories"/ "memories.jsonl"
    rows=[]
    try:
        for line in memlog.read_text(encoding="utf-8").splitlines():
            try:
                rec=json.loads(line)
                if rec.get("type")=="journal":
                    rows.append((rec.get("timestamp",""), (rec.get("text") or "").strip()))
            except Exception:
                pass
    except FileNotFoundError:
        rows=[]
    rows.sort(key=lambda x: x[0])
    out = "No journal entries found." if not rows else f"Last journal ({rows[-1][0]}):\n- {_one_line(rows[-1][1])[:160]}"
    _set_last_detail(rows[-1][1] if rows else '')
    with st.chat_message("assistant"): st.markdown(out)
    st.session_state.messages.append({"role":"assistant","content":out})
    raise SystemExit


# VALUES_FASTPATH
if any(x in low for x in ("what are my values","tell me my values","my values")):
    f = Path.home()/ "ak"/ "data"/ "identity"/ "values.md"
    if not f.exists():
        out = "(no values.md file)"
    else:
        lines = [ln.strip(" -*") for ln in f.read_text(encoding="utf-8").splitlines() if ln.strip()]
        out = "\n".join("- "+ln[:120]+("â€¦" if len(ln)>120 else "") for ln in lines[:6]) if lines else "(empty)"
    with st.chat_message("assistant"): st.markdown(out)
    st.session_state.messages.append({"role":"assistant","content":out})
    raise SystemExit


# BELIEFS_FASTPATH
if any(x in low for x in ("what are my beliefs","tell me my beliefs","my beliefs","what do i believe")):
    f = Path.home()/ "ak"/ "data"/ "identity"/ "beliefs.md"
    if not f.exists():
        out = "(no beliefs.md file)"
    else:
        lines = [ln.strip(" -*") for ln in f.read_text(encoding="utf-8").splitlines() if ln.strip()]
        out = "\n".join("- "+ln[:120]+("â€¦" if len(ln)>120 else "") for ln in lines[:6]) if lines else "(empty)"
    with st.chat_message("assistant"): st.markdown(out)
    st.session_state.messages.append({"role":"assistant","content":out})
    raise SystemExit


# WHOIS_FASTPATH
if any(x in low for x in ("who is ak","tell me about ak","tell me more about ak")):
    # detect "in N lines"
    m = re.search(r'\b(?:in|within)\s+(\d+)\s+lines?\b', low) or re.search(r'\b(\d+)\s+lines?\b', low)
    n = int(m.group(1)) if m else None
    name, role = "AK", "Artist"
    if n and n > 1:
        bullets = [
            f"- {name} â€” {role}; builds AKtivate.ai (Second Brain).",
            "- Voice: witty, precise, kind; bias to action.",
            "- Values: speed, integrity, impact; freedom/purpose/curiosity.",
            "- Heuristic: act fast; test & adjust; prefer reversible steps.",
            "- Identity focus: Artist; blends creativity with systems."
        ][:n]
        out = "\n".join(bullets)
    elif "more" in low:
        out = f"- {name} â€” {role}; builds AKtivate.ai (Second Brain).\n- Voice: witty, precise, kind; bias to action."
    else:
        out = f"{name} â€” {role}; builds AKtivate.ai (Second Brain)."
    with st.chat_message("assistant"): st.markdown(out)
    st.session_state.messages.append({"role":"assistant","content":out})
    raise SystemExit

low = prompt.strip().lower()
# OWNER_FASTPATH
if any(x in low for x in ("who owns you","who is your owner","owner of you","who owns ak")):
    out = "Owner: AK"
    with st.chat_message("assistant"): st.markdown(out)
    st.session_state.messages.append({"role":"assistant","content":out})
    raise SystemExit
# NAME_FASTPATH
if any(x in low for x in ("confirm my name","what is my name","who am i","what's my name")):
    out = "Name: AK"
    with st.chat_message("assistant"): st.markdown(out)
    st.session_state.messages.append({"role":"assistant","content":out})
    raise SystemExit


sys = ("You are AK's Second Brain. Reply like AK: witty, precise, kind. "
       "Default ONE line. No headings. No filler.")
## PROMPT_OVERRIDE
sys = ("You are AK's Second Brain. Voice: witty, precise, kind. Default to ONE line. If the user says \"in N lines\" or \"bullets\", obey exactly. No headings. No fluff.")
msgs=[{"role":"system","content":sys}] + st.session_state.messages[-16:]
## DEBUG_WRAP_START
try:
    resp = chat_llm(msgs)
full = resp.strip() if isinstance(resp,str) else str(resp)
# LLM_GUARD: if looks like error, surface it directly
if full.lower().startswith('llm http') or 'empty response' in full.lower():
    with st.chat_message('assistant'): st.markdown(full)
    st.session_state.messages.append({'role':'assistant','content':full})
    raise SystemExit
    st.session_state['last_full'] = full
    out_lines = [ln for ln in full.splitlines() if ln.strip()]
    reply = (out_lines[0][:200]+"â€¦") if out_lines else "â€¦"
    with st.chat_message("assistant"): st.markdown(reply)
    st.session_state.messages.append({"role":"assistant","content":reply})
except Exception as e:
    err = f"LLM call failed: {type(e).__name__}: {e}"
    with st.chat_message("assistant"): st.markdown(err)
    st.session_state.messages.append({"role":"assistant","content":err})
## DEBUG_WRAP_END
